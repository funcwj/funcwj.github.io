<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/wujian-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/wujian-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/themes/blue/pace-theme-minimal.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.funcwj.cn","root":"/","scheme":"Gemini","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本篇Blog虽然看起来像极了新闻稿，但实际上只是自己想做一个E2E的梳理（顺便交了某课程大作业），毕竟这玩意最近这么火。有一部分论文自己只看个大概，说法未必准确，各位看观小心~。 语音识别（Automatic Speech Recognition，ASR）是要完成输入音频的采样序列到字符序列的映射任务。传统识别模块主要包含声学模型，字典，语言模型三个部分，其中声学模型用于将输入特征转化为声学单元的">
<meta property="og:type" content="article">
<meta property="og:title" content="Overview of E2E Methods">
<meta property="og:url" content="https://www.funcwj.cn/2018/03/13/end-to-end-overview/index.html">
<meta property="og:site_name" content="WJ&#39;s site">
<meta property="og:description" content="本篇Blog虽然看起来像极了新闻稿，但实际上只是自己想做一个E2E的梳理（顺便交了某课程大作业），毕竟这玩意最近这么火。有一部分论文自己只看个大概，说法未必准确，各位看观小心~。 语音识别（Automatic Speech Recognition，ASR）是要完成输入音频的采样序列到字符序列的映射任务。传统识别模块主要包含声学模型，字典，语言模型三个部分，其中声学模型用于将输入特征转化为声学单元的">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2018-03-13T15:49:21.000Z">
<meta property="article:modified_time" content="2020-03-16T15:54:29.459Z">
<meta property="article:author" content="WJ">
<meta property="article:tag" content="Attention">
<meta property="article:tag" content="E2E">
<meta property="article:tag" content="CTC">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://www.funcwj.cn/2018/03/13/end-to-end-overview/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>Overview of E2E Methods | WJ's site</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?01edb048a0d71e5d0a00ae47bdb0dbe2";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">WJ's site</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">我只是好奇</p>
  </div>

  <div class="site-nav-right"></div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>站点首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于作者</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>文章归档<span class="badge">85</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>文章分类<span class="badge">9</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>站点标签<span class="badge">73</span></a>

  </li>
        <li class="menu-item menu-item-footprint">

    <a href="/footprint/" rel="section"><i class="fa fa-fw fa-map-marker"></i>足迹地图</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>文章搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    

  <a href="https://github.com/funcwj" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.funcwj.cn/2018/03/13/end-to-end-overview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.JPG">
      <meta itemprop="name" content="WJ">
      <meta itemprop="description" content="彷徨乎无为其侧，逍遥乎寝卧其下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WJ's site">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Overview of E2E Methods
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-03-13 23:49:21" itemprop="dateCreated datePublished" datetime="2018-03-13T23:49:21+08:00">2018-03-13</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ASR/" itemprop="url" rel="index">
                    <span itemprop="name">ASR</span>
                  </a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本篇Blog虽然看起来像极了新闻稿，但实际上只是自己想做一个E2E的梳理（顺便交了某课程大作业），毕竟这玩意最近这么火。有一部分论文自己只看个大概，说法未必准确，各位看观小心~。</p>
<p>语音识别（Automatic Speech Recognition，ASR）是要完成输入音频的采样序列到字符序列的映射任务。传统识别模块主要包含声学模型，字典，语言模型三个部分，其中声学模型用于将输入特征转化为声学单元的后验概率，字典用于表示声学单元序列到字符的映射关系，而语言模型则用于表示字符上下文之间的条件概率。由于声学模型的训练需要预知声学单元和输入特征之间的对齐信息，而常见的声学单元，比如CD-state，CD-phone等的时间对齐信息无法直接从抄本中的字符序列中获知。因此，DNN等声学模型的训练需要以传统的HMM-GMM模型进行启动引导，获得帧级别的状态标签，这一步操作称为对齐。也正是由于CD-state之类的建模单元颗粒度太小，无法直接转化成字符级别的输出，因此，需要融合字典，语言模型等信息，信息融合在声学解码器中进行。</p>
<a id="more"></a>
<p>因为抄本的长度往往小于特征序列的长度， 所以，实现特征到序列直接映射的核心在于如何处理这种对齐关系。传统的NN-HMM框架正是无法进行这种不等长的序列映射，因此才需要对齐和解码。很显然，我们期望的是一种更加自然的模型结构，可以直接以抄本作为label完成训练，直接以字/词作为输出单元，从而简化训练和解码流程。2012年之后，随着传统声学建模技术的逐渐成熟，国内外学者和研究机构开始基于语音识别这种序列映射的特性，借鉴图像，机器翻译领域的一些成功案例，开始尝试端到端（End-to-End，E2E）的建模方法。</p>
<p>为了保持脉络清晰，本部分会顺着时间线介绍三种比较成熟的端到端的建模方法：CTC<sup>[13][14]</sup>（Connectionist Temporal Classification）RNN Transducer<sup>[12]</sup>，Attention<sup>[5][9][10]</sup>机制及其在声学建模中的应用，中间会穿插一些分析和讨论，帮助理清思路。有些文章中会以Sequence to Sequence（Seq2Seq）的概念表达声学建模中端到端的含义，在这里统一称为E2E。需要注意一点，本节所述方法的输入均为声学特征，并非直接基于原始采样信号（raw waveform）建模。</p>
<h3 id="CTC"><a href="#CTC" class="headerlink" title="CTC"></a>CTC</h3><p>最早被提出用于E2E训练的是Alex Graves在2006年提出的CTC准则<sup>[13]</sup>，当时用于处理一些输入和标签不等长的问题中，比如手写识别，语音识别等等。本质上说，CTC只是一个定义在序列上的损失函数，而非一种新的网络模型。传统声学模型是一个分类器，其损失函数交叉熵是一个定义在帧级别上的度量函数，最大化当前标签被分类正确的概率，并不能很好的反映网络输出的序列特性，而CTC将句子级别的对齐信息融合在了损失函数中，通过最大化所有和和抄本对齐序列的概率和，实现E2E的模型训练，这种方式由于包含了显式的对齐计算，后来也常常称之为硬对齐（hard-alignment）。</p>
<p>对齐路径$\mathbf{\pi}$和抄本$\mathbf{y}$和是多对一关系，为了更好的描述这种关系，Graves额外引入了blank标签$\epsilon$的概念，用于隔断不同字符，比如在$T = 5$的约束下，抄本${a,b}$的对齐序列可以是${a\epsilon b\epsilon \epsilon}$，${a\epsilon\epsilon \epsilon b}$，${\epsilon ab\epsilon \epsilon}$等等。用函数$\mathcal{F}$描述$\pi \to \mathbf{y}$映射关系为$\mathcal{F}(\pi) = \mathbf{y}$。若定义输入特征序列$\mathbf{x} = \{\mathbf{x}_1, \mathbf{x}_1, \cdots, \mathbf{x}_T\}$，那么CTC损失准则表达为：</p>
<script type="math/tex; mode=display">
    \mathcal{L}_{\text{ctc}}(\mathbf{y}|\mathbf{x}) = \sum_{\pi \in \mathcal{F}^{-1}(\mathbf{y})} P(\pi | \mathbf{x})</script><p>考虑$\pi \in \mathcal{F}^{-1}$的元素呈指数级增长，故在实际中采用动态规划原理，即前向-后向算法计算$\mathcal{L}_{\text{ctc}}$。为了计算$P(\pi | \mathbf{x})$，Graves引入假设：在不同时刻，模型的输出概率相互独立，那么根据条件概率公式，有</p>
<script type="math/tex; mode=display">
    P(\pi|\mathbf{x}) = \prod_{t = 1}^T P(y_{\pi}^t | \mathbf{x}_{1 \cdots t})</script><p>其中$P(y_{\pi}^t | \mathbf{x}_{1 \cdots t})$用RNN的输出层概率表示，需要注意的是，由于引入了blank符号$\epsilon$，实际网络建模中输出层节点需要在原建模单元个数之上加1，比如在TIMIT数据上，61个音素单元的输出层个数应为62。网络训练时，用梯度下降法最小化$-\mathcal{L}_{\text{ctc}}$。2006年Graves提出CTC时，用BLSTM建模获得了30.51%的PER，超越了传统的BLSTM-HMM（33.84%）方法。网络收敛时候，各个符号之间被blank隔断，输出概率分布呈现尖峰特性，因此，通过简单的greedy-search或者beam-search方法即可完成序列解码。</p>
<p>不过，CTC最大的诟病在于Graves为了计算$P(\pi|\mathbf{x})$引入的假设，因为无论从声学特性还是语言模型上说，相邻时刻的输出概率往往是极大相关的，因此，后续的其他方法往往会消除这样的假设。</p>
<h3 id="CTC-based-System"><a href="#CTC-based-System" class="headerlink" title="CTC based System"></a>CTC based System</h3><p>CTC被提出之后产生了很多成功的应用案例，结合不断改进的RNN<sup>[27]</sup>，CNN<sup>[32]</sup>用于声学建模的思路不断出现，比较典型的算是百度硅谷研究院的Deep Speech<sup>[2][16]</sup>系列。</p>
<p>Deep Speech 1，2是均以CTC准则构建的端到端识别系统。2014年，Deep Speech 1公布，主体上沿用Graves等人的建模思路，但是在声学模型上做了简化。前三层为使用clipped ReLU（$g(z) = \min \{ \max \{0, z\}, 20 \}$）作为激活函数的全连接网络，第四层采用双向RNN，第五层为全连接层，接受双向RNN的输出，输出层使用CTC作为误差准则。配合数据抖动和dropout等正则化优化技巧，Deep Speech 1最终在SWB+FSH 2000h数据集上超越了当时传统方法最好的开源结果。</p>
<p>2014年到2016年之间，CNN<sup>[1][26]</sup>以及BatchNorm<sup>[1]</sup>等正则化方法相继被引入声学建模中，并取得了很好的结果。Deep Speech 2在2016年公开，和DS1相比，声学模型中加入了如下新的特性：</p>
<ul>
<li>引入卷积层用于特征抽取，替代之前的全连接层，在时域和频域的二维卷积可以明显增强声学模型在噪声环境下的识别鲁棒性</li>
<li>RNN部分采用sequence-wise的BatchNorm，用于加速网络收敛，并且发现，随着网络层数的加深，对收敛度的提升越好</li>
<li>使用Cho等人在2014年提出的GRU代替普通RNN，相比LSTM，GRU<sup>[8]</sup>可以获得相近的结果，同时计算复杂度更小</li>
<li>在GRU层之上加入lookahead卷积层，用于获取一些future context。</li>
</ul>
<p>DS2在普通话和英语上同时取得了可观的结果，在普通话带噪测试集上，使用了BatchNorm和2D卷积的模型相比浅层的RNN在WER上有了48%的相对提升，并且在voice query数据上超越了人类水平。</p>
<p>Google在2017年提出的Neural Speech Recognizer<sup>[29]</sup>也是以CTC为准则的识别系统。NSR采用双向LSTM建模，在超过12万小时的数据上进行训练，对比了CD-phone和word两种建模单元，在YouTube转写任务上，以word作为建模单元的NSR超越了传统CD-phone的ASR效果。</p>
<p>在开源社区CTC也相当活跃，Miao等人基于Kaldi语音识别工具包开源了eesen<sup>[20]</sup>，满足了CTC和传统声学解码器的耦合，Baidu开源了社区效率最高的CTC实现<a href="https://github.com/baidu-research/warp-ctc" target="_blank" rel="noopener">warp-ctc</a>，在同等的计算量下，其耗时远低于其他工具包，Facebook研究院开源了他们基于CTC的端到端识别工具<a href="https://github.com/facebookresearch/wav2letter" target="_blank" rel="noopener">wav2letter</a><sup>[11]</sup>，CUDNN7.0中也增加了CTC的API接口。此外，受到CTC的启发，Dan等人提出的Lattice Free MMI（LF-MMI，chain model）<sup>[22]</sup>获得巨大成功，一方面降低了区分性训练的耗时，另一方面可以获得8%的相对提升，被誉为声学模型近几年最大的创新。</p>
<h3 id="RNN-Transducer"><a href="#RNN-Transducer" class="headerlink" title="RNN Transducer"></a>RNN Transducer</h3><p>为了进一步提升CTC的表现，Graves后来提出了RNN Transducer<sup>[12]</sup>结构，用于修正CTC在计算序列概率中的假设缺陷。思路是保留原CTC声学模型（称为转录网络）的同时，引入一个额外的网络，称为为预测网络，用于对抄本序列的输出进行预测，起到类似语言模型的作用。在$t$时刻，当前符号为$u$时，网络输出符号$k$的概率表示为：</p>
<script type="math/tex; mode=display">
    P(k | t, u) = \frac{\exp(\mathbf{f}_t^k + \mathbf{g}_u^k)}{\sum_{k'} \exp(\mathbf{f}_t^{k'} + \mathbf{g}_u^{k'})}</script><p>其中$\mathbf{f}_t,\mathbf{g}_u$表示转录和预测网络的输出概率向量。训练时，预测网络的输入源自抄本序列，解码时，预测网络的输入来自转录网络的输出，输入采用one-hot编码的形式，因此，在RNN Transducer中，$P(\pi|\mathbf{x})$的计算公式变为：</p>
<script type="math/tex; mode=display">
    P(\pi | \mathbf{x}) = \prod_{t=1}^T P(y_{\pi}^t | \mathbf{x}_{1 \cdots t}, \pi_{\{1,\cdots,t\}})</script><p>从这里可以看出，由于$t$时刻的输出$y<em>{\pi}^t$会作为预测网络的输入，因此，$t + 1$时刻的输出$y</em>{\pi}^{t+1}$不再和$y_{\pi}^t$相互独立，这种条件更加符合语音上下文之间的相关性。实验中，一层128节点的预测网络和两层128节点的转录网络在TIMIT上取得了23.2%的PER，相比纯转录网络（25.5%），降低了2.3%个百分点。</p>
<p>在2013年，Graves用多层LSTM建模<sup>[15]</sup>，并用CTC网络的权值初始化转录网络，在TIMIT上取得了17.7%的PER，成为当时最好的结果，而同结构的CTC结果为18.6%。研究同时表明：</p>
<ul>
<li>LSTM的建模能力远远超越普通RNN</li>
<li>网络走向深度的收益好于扩展宽度</li>
<li>双向网络的建模能力胜于单向网络</li>
</ul>
<p>RT的问题在于，转录网络和预测网络除了通过$P(k|u, t)$进行信息融合之外，并不相互依赖，因此，二者较为独立。其次，RT依旧保持了CTC设计中硬对齐部分，用于计算损失函数，在这点上计算复杂度较高，本质上说，属于对CTC的改进。</p>
<h3 id="Encoder-Decoder-Structure"><a href="#Encoder-Decoder-Structure" class="headerlink" title="Encoder-Decoder Structure"></a>Encoder-Decoder Structure</h3><p>在提Attention机制之前需要先说一下Encoder-Decoder结构。Encoder-Decoder是Cho等人在2014年提出的一种包含两个RNN的网络结构<sup>[8]</sup>，最初用于机器翻译，也正是在这篇论文中，他们提出了LSTM的简化版本GRU（Gated Recurrent Unit）。</p>
<p>在E-D结构中，编码器用于将输入的变长序列$\mathbf{x}$编码成定长表示$\mathbf{c}$，而解码器用于将此定长表示解码成另一种符号序列$\mathbf{y}$，两个网络做联合训练，最大化条件概率$P(\mathbf{y} | \mathbf{x})$，以此完成序列映射。一般的，$\mathbf{c}$用encoder扫描一遍$\mathbf{x}$之后的hidden state表示。对于decoder，在生成$y_t$时，接受上一时刻的输出$y_{t - 1}$和$\mathbf{c}$作为输入，hidden state的更新表示为：</p>
<script type="math/tex; mode=display">
    \mathbf{s}_t = \mathcal{R}(\mathbf{s}_{t - 1}, y_{t - 1}, \mathbf{c})</script><p>在生成$t$时刻生成$y_t$的条件概率$P(y_t | y_{1 \cdots (t - 1)}, \mathbf{x})$表示为：</p>
<script type="math/tex; mode=display">
    P(y_t | y_{1 \cdots (t - 1)}, \mathbf{x}) = \mathcal{G}(\mathbf{s}_t, y_{t - 1}, \mathbf{c})</script><p>其中$\mathcal{G}$可以用一个带有softmax输出层的MLP表示。在E-D结构下，decoder生成序列$\mathbf{y}$的条件概率可以根据条件概率公式得到：</p>
<script type="math/tex; mode=display">
    P(\mathbf{y} | \mathbf{x}) = \prod_t P(y_t |y_{<t}, \mathbf{x}) = \prod_{t} P(y_t | y_{1 \cdots (t - 1)}, \mathbf{c})</script><p>通过引入编码器，使得decoder的输出不再直接依赖于输入$\mathbf{x}$，生成序列的长度也只取决于解码的步数，这是这种结构能够很好的处理变长序列映射问题的关键。但是这种结构会带来两个很明显的问题：</p>
<ul>
<li>由于RNN的记忆遗忘问题，实际中编码器将输入序列全部编码成定长表示会造成表达能力不足以及信息丢失等问题，这种问题往往随着输入序列的增长而愈加明显。</li>
<li>即使全部信息被编码进定长表示，在解码阶段，未必每一步都需要全部的输入信息，比如关联最大的可能仅仅和输入序列对齐部分的上下文区间。</li>
</ul>
<p>正是出于这种考虑，一种称为attention机制的encoder-decoder结构被提出。这种结构摒弃了编码器输出定长编码的限制，将编码器hidden state的加权和输入decoder，权重由网络自身学习得到。这种结构一来避免了长时输入造成的信息丢失，同时允许decoder自行学习注意的内容，更加符合实际。attention最早被应用于机器翻译<sup>[3]</sup>，物体追踪，图像主题生成，后来被Cho等人用于语音识别<sup>[9][10]</sup>，并取得成功。</p>
<h3 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h3><p>在引入attention机制的encoder-decoder框架中，encoder用于将输入特征$\mathbf{x}_{1 \cdots T}$转换为高层次的表示特征$\mathbf{h}_{1 \cdots U}$，decoder用于根据表示特征预测序列单元$\mathbf{y}_t$，编码器和解码器之间通过attention机制关联。attention的作用是根据decoder的状态$\mathbf{s}_t$，结合$\mathbf{h}$计算attention context $\mathbf{c}_t$，帮助解码器预测输出$\mathbf{y}_t$。</p>
<p>现在对上述过程进行符号化，不同于纯粹的E-D结构，在生成$t$时刻生成$y_t$的条件概率$P(y_t | y_{1 \cdots (t - 1)}, \mathbf{x})$在引入attention机制之后变为：</p>
<script type="math/tex; mode=display">
    P(y_t | y_{1 \cdots (t - 1)}, \mathbf{x}) = \mathcal{G}(\mathbf{s}_{i}, y_{t - 1}, \mathbf{c}_i)</script><p>attention context $\mathbf{c}_i$为表示特征$\mathbf{h}_{1 \cdots U}$的加权和，用$\alpha_{ij}$表示权值，$\mathbf{c}_i = \sum_{j = 1}^U \alpha_{ij} \mathbf{h}_j$，这里引入的$\alpha_i$就是attention weight，其计算过程可以统一表示为：</p>
<script type="math/tex; mode=display">
\begin{align}
    e_{ij} & = \mathcal{A}(\mathbf{s}_i, \mathbf{h}_j, \alpha_{i -1}) \\
    \alpha_i &= \text{softmax}(e_i)
\end{align}</script><p>$e_{ij}$称为scaler energy，不同的attention其计算过程不同。下面介绍几种常见的attention类型：</p>
<ol>
<li><p>MLP attention<sup>[9]</sup>。<br> 用一个多层感知机（线性网络）表示$\mathcal{A}$的计算过程称为MLP attention，ASR中最早被Cho等人在其研究中使用，输入为向量$\mathbf{s}_i$和$\mathbf{h}_j$的拼接。</p>
</li>
<li><p>Tanh attention<sup>[10]</sup>。<br> Tanh attention又称为content-based attention，最早在机器翻译中使用，Cho等人在2015年提出的ARSG（Attention-based Recurrent Sequence Generator）中借鉴了这种计算方式，提出一种location-aware的计算方法，考虑了上一步生成的attention权值信息$\alpha_{i - 1}$，计算表示如下：</p>
<script type="math/tex; mode=display">
     e_{ij} = w^\top \tanh(\phi(\mathbf{s}_i) + \psi(\mathbf{h}_j) + \theta(\mathbf{f}_{ij}))</script><p> 其中$w$为权值向量，$\phi(\cdot), \psi(\cdot), \theta(\cdot)$均为MLP网络。$\mathbf{f}_i$为一个矩阵，用$\alpha_{i - 1}$和矩阵$\mathbf{F}$卷积得到：</p>
<script type="math/tex; mode=display">
        \mathbf{f}_i = \mathbf{F} * \alpha_{i - 1}</script></li>
<li><p>Dot attention<sup>[5]</sup>。<br> Dot attention是Google Brain团队在LAS（Listen Attend and Spell）结构中使用的计算方法，通过两个MLP网络$\phi(\cdot), \psi(\cdot)$将$\mathbf{s}_i$和$\mathbf{h}_j$embedding成等长向量，二者做点积：</p>
<script type="math/tex; mode=display">
        e_{ij} = \langle \phi(\mathbf{s}_i), \psi(\mathbf{h}_j) \rangle</script><p>  实验表明，在Google voice search traffic任务上，dot-attention的表现比tanh-attention要好。</p>
</li>
<li><p>Multi-Head attention<sup>[30]</sup>。<br>  前面的几种attention计算的共同点在于用$\mathbf{h}$的加权平均作为attention context，这种方式称为single-head attention，scaler energy依赖单一的$\mathbf{s}_i$。multi-head attention（MHA）的机制是Google Brain团队在2017年提出的概念，首先被应用于机器翻译（NMT）。它将$\mathbf{s}_i$做投影变换，产生$M$个embedding，基于此计算出$M$个scaler energy，彼此之间分布不同，最后将各自的attention context拼接成最终的context向    量。这种方式有助于减少context对encoder信息的依赖，同时由于每支head可以从$\mathbf{h}$中提取不同的信息，系统鲁棒性更强。借助上面的符号定义，其计算过程可以表示为：</p>
<script type="math/tex; mode=display">
  \begin{align}
      \mathbf{c}_i &= \mathcal{A}(\mathbf{W}_i \mathbf{s}_i, \mathbf{h}) \\
      \mathbf{c} &=\text{concat}(\mathbf{c}_{1 \cdots M}) \mathbf{W}_o
  \end{align}</script><p>  其中$\mathbf{W}_i$表示变换矩阵，$\mathbf{W}_o$用于减少向量拼接之后的维度。</p>
</li>
</ol>
<p>Attention机制在机器翻译中取得成功之后，被引入语音识别，处理声学特征到抄本之间的序列建模。从2015年开始，Attention based方法逐渐成为研究热点。</p>
<h3 id="Attention-based-Models"><a href="#Attention-based-Models" class="headerlink" title="Attention based Models"></a>Attention based Models</h3><p>2014年，attention机制在TIMIT上最早的尝试取得了18.61%的PER<sup>[9]</sup>。随后，Cho等人提出了ARSG（Attention-based Recurrent Sequence Generator）<sup>[10]</sup>，采用location-aware的attention替换早期的MLP-attention，在TIMIT数据集上获得了17.6%的PER（Phone Error Rate），这一结果已经超越了2013年RNN Transducer的17.7%<sup>[15]</sup>。</p>
<p>谷歌同年提出的LAS（Listen Attend and Spell）<sup>[5]</sup>整体与ARSG类似，不过更加结构化。LAS中encoder称为Listener，decoder称为AttendAndSpeller。Listener是一个金字塔结构的BLSTM-encoder，这种形式可以有效减少表示特征的输出步长，加速网络收敛。Speller是一个两层的LSTM，与ARSG不同的是，attention context采用dot attention计算。LAS的评估在Google voice search任务上进行，和传统方法最好的结果（CLDNN 8.0%）相比，配合语言模型重打分（LM rescore），取得10.3%的WER。</p>
<p>Baidu的Deep Speech 3没有单独的进行模型设计<sup>[4]</sup>，而是从数据，编码器结构，解码配置等方面详细对比了CTC，RNN Transducer和 Attention based方法。在不借助语言模型辅助的条件下，在switchboard数据集上分别取得9.0%，8.5%和8.6%的WER，在更加真实的DeepSpeech数据上，三者的最优表现则较为一致。</p>
<h3 id="Attention-vs-CTC"><a href="#Attention-vs-CTC" class="headerlink" title="Attention vs CTC"></a>Attention vs CTC</h3><p>相比CTC，attention机制更希望attention layer自身学习到对齐信息，用于辅助decoder进行序列预测。训练时的损失度量依旧是传统声学建模的交叉熵，因此，相比CTC具有简洁性。这种方法称为软对齐（soft-alignment）。$\alpha$表示网络学习到的对齐信息，网络收敛之后，其分布往往比较尖锐。</p>
<p>而CTC则是通过显示的计算对齐信息，用于损失函数设计实现端到端的训练，计算复杂度较高。做推断时，输出序列的时序长度和输入一致，而E-D框架中，decoder的输出则没有这一限制，理论上可能是任意长度。对于RNN Transducer，转录网络和预测网络之间仅仅通过输出层做信息耦合（做硬对齐的损失计算），而网络之间的状态信息也没有交互，在这点上没有E-D框架耦合性高。</p>
<p>解码方面，由于CTC的输出分布呈现尖峰特性，大部分时长被blank符号填充，因此，虽然没有在学习过程中学习语言建模，但是也可以采用greedy/beam search的方法进行解码。如果采用细粒度的建模方法，比如CI-phone，也可以使用声学解码器进行解码。</p>
<p>CTC相比attention更易于实现online解码，只需要将声学模型替换为单向RNN（LSTM \&amp; GRU etc.）。而E-D框架中，由于encoder需要扫描一遍输入序列，因此，实时性较差。关于如何进行online的改进，陆续有学者提出了自己的方案进行相关改进。下一部分会介绍其中一种思路。</p>
<h3 id="Online-Attention"><a href="#Online-Attention" class="headerlink" title="Online Attention"></a>Online Attention</h3><p>上面提到的attention也常常被称为full-sequence attention，因为在计算scaler energy时需要利用到整个表示特征$\mathbf{h}_{1\cdots U}$。由此带来的问题是，decoder需要等待encoder完成全部编码表示才能工作，也就意味着decoder无法在线/流式工作，这极大的限制了其在语音交互中的应用。因此，如何进行在线的改善attention模型成为拓展其应用场景必须解决的问题。</p>
<p>Google Brain在2016年提出的Netural Transducer（NT）<sup>[18]</sup>将attention计算的context限制在事先划分的语音段中，假设段长$W$，则$T$帧的数据可以划分为$B = [\frac{T}{W}]$段。在每个块中，NT产生$k$个输出符号，并且强制最后一个符号为$e$，表示该语音段中已经产生完所有输出。根据以上定义，第$b$段语音对应的输出序列$y_{e_b -1 \cdots e_b}$产生的条件概率为：</p>
<script type="math/tex; mode=display">
P(y_{e_b - 1 \cdots e_b} | \mathbf{x}_{1 \cdots bW}) = \prod_{i = e_{b - 1} + 1}^{e_b} P(y_m | \mathbf{x}_{1 \cdots bW}, y_{1 \cdots (i-1)})</script><p>其中$\mathbf{x}_{1 \cdots bW}$和$y_{1 \cdots (i-1)}$分别表示已经观测到的特征和NT的当前输出序列。而scale energy和attention context的计算仅仅只在当前语音段的表示特征$\mathbf{h}_{(b-1)W \cdots bW}$上进行，即$\mathbf{c}_i = \sum_{j = 1}^W \alpha<em>{ij} \mathbf{h}</em>{(b - 1)W + j}$，其中：</p>
<script type="math/tex; mode=display">
\alpha_i = \mathcal{A}(\mathbf{s}_i, \mathbf{h}_{(b - 1)W + j})</script><p>关于attend的具体实现，论文中提出了三种思路，除了LAS中的dot attention之外，还有MLP attention和LSTM attention，即用一个多层感知机或者LSTM网络来计算scale energy。通过调节$W$的值，可以发现LSTM attention的结果更加连贯，配合一个三层的BLSTM-encoder，在TIMIT上可以取得18.2%的PER，和full-sequence attention 17.6%相比，这个结果是可观的。后来文献中常将NT实现online的方法称为limited-sequence attention。</p>
<p>在ARSG中，作者也分析了full-sequence attention容易受到注意力丢失问题的影响，在长句子上的表现普遍不佳。NT中划分语音段的方式帮助模型中的attention前向移动，因此，对此问题的敏感有所降低。但是在更加复杂的任务上，比如Google的voice search，流式/在线的Netural Transducer的表现不如离线的LAS，因此，Google的speech team将NT的思路应用于LAS中，并在原先LAS的设计上做了一些优化工作<sup>[7][25]</sup>，主要包括如下几点：</p>
<ul>
<li>向前拓展注意力计算的context，即回顾若干（$k$）个语音段，同时向后拓展5帧，即将$\mathbf{h}_{(b-1)W + 1, \cdots, bW} $由$\text{Listen}(\mathbf{x}_{(b-1)W + 1, \cdots, bW})$修正为$\text{Listen}(\mathbf{x}_{(b-k)W + 1, \cdots, bW + 5})$，通过引入少量的延时，重复利用之前的历史信息增强了attention信息含量。</li>
<li>使用原先的LAS模型参数初始化LAS-NT。</li>
<li>参照机器翻译中的相关经验，使用字片替代原来的字建模，同时在解码过程中融合一个语言模型。</li>
</ul>
<p>实验结果表明，在Google voice search traffic任务，1,2的改进可以使得single-head NT获得和single-head LAS相媲美的结果（9.9% vs 9.8%），结合3，multi-head NT取得了和multi-head LAS相同的结果（8.6%）。到此，attention具有了在实际场景中部署的基础。</p>
<p>另外一种online改进的思路则是借鉴CTC中hard-alignment的思路，假设网络的对齐是单调的，即注意力沿着时间轴转移，以Google Brain Raffel<sup>[6][24]</sup>等人为代表。目前实际的表现尚不如NT-LAS，Google团队正在进行相关调优工作。</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>本文从2006年被提出的CTC准则出发，依次介绍了RNN Transducer，encoder-decoder框架三种用于端到端声学建模的方法，同时梳理了三者之间的关系及区别，并参阅了近四年来的相关文献，展示了Google，Baidu等语音团队在端到端方向上的实践思路。事实上，关于E2E的实践还远不止本文所述，比如结合attention的LF-MMI<sup>[21]</sup>，基于VDNN（Very Deep Neural Network）<sup>[31]</sup>的建模实践，基于CTC准则的encoder-decoder框架<sup>[28]</sup>，结合RNN Transducer的attention机制<sup>[23]</sup>，CTC attention的联合训练以及CE-CTC的联合训练<sup>[19]</sup>等等。整体来说，端到端是语音领域近两年比较火热的一个方向，由于在真实复杂的场景（噪声，混响，多说话人等等）下，其实际的声学鲁棒性尚不能媲美传统方案，因此还有很多难关等待被攻克。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, and G. Penn. Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 4277–4280. IEEE, 2012.<br>[2] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al. Deep speech 2: End-to-end speech recognition in english and mandarin. In International Conference on Machine Learning, pages 173–182, 2016.<br>[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.<br>[4] E. Battenberg, J. Chen, R. Child, A. Coates, Y. Gaur, Y. Li, H. Liu, S. Satheesh, D. Seetapun, A. Sriram, et al. Exploring neural transducers for end-to-end speech recognition. arXiv preprint arXiv:1707.07413, 2017.<br>[5] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals. Listen, attend and spell. arxiv preprint. arXiv preprint arXiv:1508.01211, 1(2):3, 2015.<br>[6] C.-C. Chiu and C. Raffel. Monotonic chunkwise attention. arXiv preprint arXiv:1712.05382, 2017.<br>[7] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, K. Gonina, et al. State-of-the-art speech recognition with sequence-to-sequence models. arXiv preprint arXiv:1712.01769, 2017.<br>[8] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder- decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.<br>[9] J. Chorowski, D. Bahdanau, K. Cho, and Y. Bengio. End-to-end continuous speech recognition using attention-based recurrent nn: First results. arXiv preprint arXiv:1412.1602, 2014.<br>[10] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio. Attention-based models for speech recognition. In Advances in neural information processing systems, pages 577–585, 2015.<br>[11] R. Collobert, C. Puhrsch, and G. Synnaeve. Wav2letter: an end-to-end convnet-based speech recognition system. CoRR, abs/1609.03193, 2016.<br>[12] A. Graves. Sequence transduction with recurrent neural networks. Computer Science, 58(3):235–242, 2012.<br>[13] A. Graves and F. Gomez. Connectionist temporal classification:labelling unsegmented sequence data with recurrent neural networks. In International Conference on Machine Learning, pages 369–376, 2006.<br>[14] A. Graves and N. Jaitly. Towards end-to-end speech recognition with recurrent neural networks. In International Conference on Machine Learning, pages 1764–1772, 2014.<br>[15] A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pages 6645–6649. IEEE, 2013.<br>[16] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014.<br>[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456, 2015.<br>[18] N. Jaitly, Q. V. Le, O. Vinyals, I. Sutskever, D. Sus- sillo, and S. Bengio. An online sequence-to-sequence model using partial conditioning. In Advances in Neural Information Processing Systems, pages 5067–5075, 2016.<br>[19] S. Kim, T. Hori, and S. Watanabe. Joint ctc-attention based end-to-end speech recognition using multi-task learning. In Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on, pages 4835–4839. IEEE, 2017.<br>[20] Y. Miao, M. Gowayyed, and F. Metze. Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding. In Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Work- shop on, pages 167–174. IEEE, 2015.<br>[21] D. Povey, H. Hadian, P. Ghahremani, K. Li, and S. Khudanpur. A time-restricted self-attention layer for asr.<br>[22] D. Povey, V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar, X. Na, Y. Wang, and S. Khudanpur. Purely sequence-trained neural networks for asr based on lattice-free mmi. In Interspeech, pages 2751–2755, 2016.<br>[23] R. Prabhavalkar, K. Rao, T. N. Sainath, B. Li, L. Johnson, and N. Jaitly. A comparison of sequence-to-sequence models for speech recognition. In Proc. Interspeech, pages 939–943, 2017.<br>[24] C. Raffel, T. Luong, P. J. Liu, R. J. Weiss, and D. Eck. Online and linear-time attention by enforcing monotonic alignments. arXiv preprint arXiv:1704.00784, 2017.<br>[25] T. N. Sainath, C.-C. Chiu, R. Prabhavalkar, A. Kan- nan, Y. Wu, P. Nguyen, and Z. Chen. Improving the performance of online neural transducer models. arXiv preprint arXiv:1712.01807, 2017.<br>[26] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhadran. Deep convolutional neural networks for lvcsr. In Acoustics, speech and signal process- ing (ICASSP), 2013 IEEE international conference on, pages 8614–8618. IEEE, 2013.<br>[27] H. Sak, A. Senior, K. Rao, and F. Beaufays. Fast and accurate recurrent neural network acoustic models for speech recognition. arXiv preprint arXiv:1507.06947, 2015.<br>[28] H. Sak, M. Shannon, K. Rao, and F. Beaufays. Recurrent neural aligner: An encoder-decoder neural net- work model for sequence to sequence mapping. In Proc. of Interspeech, 2017.<br>[29] H. Soltau, H. Liao, and H. Sak. Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition. arXiv preprint arXiv:1610.09975, 2016.<br>[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000–6010, 2017.<br>[31] Y. Zhang, W. Chan, and N. Jaitly. Very deep convolutional networks for end-to-end speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on, pages 4845– 4849. IEEE, 2017.<br>[32] Y. Zhang, M. Pezeshki, P. Brakel, S. Zhang, C. L. Y. Bengio, and A. Courville. Towards end-to-end speech recognition with deep convolutional neural networks. arXiv preprint arXiv:1701.02720, 2017.</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>WJ
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://www.funcwj.cn/2018/03/13/end-to-end-overview/" title="Overview of E2E Methods">https://www.funcwj.cn/2018/03/13/end-to-end-overview/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Attention/" rel="tag"><i class="fa fa-tag"></i> Attention</a>
              <a href="/tags/E2E/" rel="tag"><i class="fa fa-tag"></i> E2E</a>
              <a href="/tags/CTC/" rel="tag"><i class="fa fa-tag"></i> CTC</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/01/05/bptt-of-rnn/" rel="prev" title="BPTT of RNN">
      <i class="fa fa-chevron-left"></i> BPTT of RNN
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/04/14/setk-speech-enhancement-tookit/" rel="next" title="SETK - Speech Enhancement Tools based on Kaldi">
      SETK - Speech Enhancement Tools based on Kaldi <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#CTC"><span class="nav-number">1.</span> <span class="nav-text">CTC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CTC-based-System"><span class="nav-number">2.</span> <span class="nav-text">CTC based System</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN-Transducer"><span class="nav-number">3.</span> <span class="nav-text">RNN Transducer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder-Decoder-Structure"><span class="nav-number">4.</span> <span class="nav-text">Encoder-Decoder Structure</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention-Mechanism"><span class="nav-number">5.</span> <span class="nav-text">Attention Mechanism</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention-based-Models"><span class="nav-number">6.</span> <span class="nav-text">Attention based Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention-vs-CTC"><span class="nav-number">7.</span> <span class="nav-text">Attention vs CTC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Online-Attention"><span class="nav-number">8.</span> <span class="nav-text">Online Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Conclusion"><span class="nav-number">9.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reference"><span class="nav-number">10.</span> <span class="nav-text">Reference</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="WJ"
      src="/uploads/avatar.JPG">
  <p class="site-author-name" itemprop="name">WJ</p>
  <div class="site-description" itemprop="description">彷徨乎无为其侧，逍遥乎寝卧其下</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">85</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">73</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/funcwj" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;funcwj" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:funcwj@foxmail.com" title="E-Mail → mailto:funcwj@foxmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/jian-fu-16" title="Zhi Hu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;jian-fu-16" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i>Zhi Hu</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.funcwj.cn/" title="Jianwu → https:&#x2F;&#x2F;www.funcwj.cn"><i class="fa fa-fw fa-google"></i>Jianwu</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://wangbaiyuan.cn/" title="http:&#x2F;&#x2F;wangbaiyuan.cn" rel="noopener" target="_blank">极客人</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://jcf94.com/" title="http:&#x2F;&#x2F;jcf94.com" rel="noopener" target="_blank">jcf94</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://hujian.website/" title="https:&#x2F;&#x2F;hujian.website" rel="noopener" target="_blank">hijkzzz</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://placebokkk.github.io/" title="http:&#x2F;&#x2F;placebokkk.github.io&#x2F;" rel="noopener" target="_blank">Yang Chao</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.cnblogs.com/xingshansi/" title="https:&#x2F;&#x2F;www.cnblogs.com&#x2F;xingshansi&#x2F;" rel="noopener" target="_blank">桂的blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://homes.esat.kuleuven.be/~dspuser/dasp/index.html" title="http:&#x2F;&#x2F;homes.esat.kuleuven.be&#x2F;~dspuser&#x2F;dasp&#x2F;index.html" rel="noopener" target="_blank">DASP</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://ccrma.stanford.edu/~jos/sasp/sasp.html" title="https:&#x2F;&#x2F;ccrma.stanford.edu&#x2F;~jos&#x2F;sasp&#x2F;sasp.html" rel="noopener" target="_blank">SASP</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://link.springer.com/book/10.1007%2F978-3-540-78612-2" title="https:&#x2F;&#x2F;link.springer.com&#x2F;book&#x2F;10.1007%2F978-3-540-78612-2" rel="noopener" target="_blank">MASP</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.audiolabs-erlangen.de/home" title="https:&#x2F;&#x2F;www.audiolabs-erlangen.de&#x2F;home" rel="noopener" target="_blank">AudioLibs</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://arxiv.org/list/eess.AS/recent" title="https:&#x2F;&#x2F;arxiv.org&#x2F;list&#x2F;eess.AS&#x2F;recent" rel="noopener" target="_blank">ASLP Arxiv</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://signalprocessingsociety.org/publications-resources/ieeeacm-transactions-audio-speech-and-language-processing" title="https:&#x2F;&#x2F;signalprocessingsociety.org&#x2F;publications-resources&#x2F;ieeeacm-transactions-audio-speech-and-language-processing" rel="noopener" target="_blank">IEEE-TASLP</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.isca-speech.org/iscaweb/index.php/archive/online-archive" title="https:&#x2F;&#x2F;www.isca-speech.org&#x2F;iscaweb&#x2F;index.php&#x2F;archive&#x2F;online-archive" rel="noopener" target="_blank">ISCA</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://ai.google/research/pubs/?area=SpeechProcessing" title="https:&#x2F;&#x2F;ai.google&#x2F;research&#x2F;pubs&#x2F;?area&#x3D;SpeechProcessing" rel="noopener" target="_blank">Google</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www-i6.informatik.rwth-aachen.de/" title="http:&#x2F;&#x2F;www-i6.informatik.rwth-aachen.de" rel="noopener" target="_blank">RWTH</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.merl.com/publications/?ystart=1991&yend=2019&sa=on" title="http:&#x2F;&#x2F;www.merl.com&#x2F;publications&#x2F;?ystart&#x3D;1991&amp;yend&#x3D;2019&amp;sa&#x3D;on" rel="noopener" target="_blank">MERL</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://ei.uni-paderborn.de/en/nt/research-mission/publications/" title="https:&#x2F;&#x2F;ei.uni-paderborn.de&#x2F;en&#x2F;nt&#x2F;research-mission&#x2F;publications&#x2F;" rel="noopener" target="_blank">Paderborn</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://bio-asplab.citi.sinica.edu.tw/p-conference.html" title="http:&#x2F;&#x2F;bio-asplab.citi.sinica.edu.tw&#x2F;p-conference.html" rel="noopener" target="_blank">ASP-LAB</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.kecl.ntt.co.jp/english/index.html" title="http:&#x2F;&#x2F;www.kecl.ntt.co.jp&#x2F;english&#x2F;index.html" rel="noopener" target="_blank">NTT-CSL</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.irisa.fr/metiss/ozerov/" title="http:&#x2F;&#x2F;www.irisa.fr&#x2F;metiss&#x2F;ozerov&#x2F;" rel="noopener" target="_blank">Alexey Ozerov</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://israelcohen.com/" title="https:&#x2F;&#x2F;israelcohen.com" rel="noopener" target="_blank">Israel Cohen</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.eng.biu.ac.il/gannot/" title="http:&#x2F;&#x2F;www.eng.biu.ac.il&#x2F;gannot&#x2F;" rel="noopener" target="_blank">Sharon Gannot</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.danielpovey.com/" title="http:&#x2F;&#x2F;www.danielpovey.com" rel="noopener" target="_blank">Daniel Povey</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://web.cse.ohio-state.edu/~wang.77/pubs_year.html" title="http:&#x2F;&#x2F;web.cse.ohio-state.edu&#x2F;~wang.77&#x2F;pubs_year.html" rel="noopener" target="_blank">Deliang Wang</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sites.google.com/view/shinjiwatanabe/publications" title="https:&#x2F;&#x2F;sites.google.com&#x2F;view&#x2F;shinjiwatanabe&#x2F;publications" rel="noopener" target="_blank">Shinji Watanabe</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sites.google.com/site/dongyu888/" title="https:&#x2F;&#x2F;sites.google.com&#x2F;site&#x2F;dongyu888&#x2F;" rel="noopener" target="_blank">Dong Yu</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.citi.sinica.edu.tw/pages/yu.tsao/publications_en.html" title="https:&#x2F;&#x2F;www.citi.sinica.edu.tw&#x2F;pages&#x2F;yu.tsao&#x2F;publications_en.html" rel="noopener" target="_blank">Yu Tsao</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sites.google.com/site/tsainath/" title="https:&#x2F;&#x2F;sites.google.com&#x2F;site&#x2F;tsainath&#x2F;" rel="noopener" target="_blank">Tara N.</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.jonathanleroux.org/" title="http:&#x2F;&#x2F;www.jonathanleroux.org" rel="noopener" target="_blank">Jonathan</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://speechlab.sjtu.edu.cn/members/yanmin_qian" title="https:&#x2F;&#x2F;speechlab.sjtu.edu.cn&#x2F;members&#x2F;yanmin_qian" rel="noopener" target="_blank">Yanmin Qian</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.microsoft.com/en-us/research/people/tayoshio/" title="https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;people&#x2F;tayoshio&#x2F;" rel="noopener" target="_blank">Takuya Yoshioka</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://scholar.google.com/citations?user=QG8aWfIAAAAJ&hl=zh-CN" title="https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?user&#x3D;QG8aWfIAAAAJ&amp;hl&#x3D;zh-CN" rel="noopener" target="_blank">Marc Delcroix</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sites.google.com/view/xuyong/home" title="https:&#x2F;&#x2F;sites.google.com&#x2F;view&#x2F;xuyong&#x2F;home" rel="noopener" target="_blank">Yong Xu</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://staff.ustc.edu.cn/~jundu/Publications.html" title="http:&#x2F;&#x2F;staff.ustc.edu.cn&#x2F;~jundu&#x2F;Publications.html" rel="noopener" target="_blank">Jun Du</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="beian"><a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">陕ICP备 17010872号-1 </a>
  </div>

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WJ</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">381k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">5:47</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.1
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
